<!DOCTYPE html>
<html>

<head>
    <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
    <link rel="stylesheet" href="https://code.getmdl.io/1.3.0/material.indigo-pink.min.css">
    <script defer src="https://code.getmdl.io/1.3.0/material.min.js"></script>

    <style>
        #videoElement {
            width: 800px;
            height: 600px;
            border-radius: 20px;
        }

        #canvasElement {
            display: none;
            width: 800px;
            height: 600px;
        }

        .demo-content {
            padding: 20px;
            display: flex;
            flex-direction: column;
            align-items: center;
        }

        .button-group {
            margin-bottom: 20px;
        }
    </style>
</head>

<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header">
        <header class="mdl-layout__header">
            <div class="mdl-layout__header-row">
                <!-- Title -->
                <span class="mdl-layout-title">Gemini Live Demo</span>
            </div>
        </header>
        <main class="mdl-layout__content">
            <div class="page-content">
                <div class="demo-content">
                    <!-- Voice Control Buttons -->
                    <div class="button-group">
                        <button id="startButton"
                            class="mdl-button mdl-js-button mdl-button--fab mdl-button--mini-fab mdl-button--colored">
                            <i class="material-icons">mic</i>
                        </button>
                        <button id="stopButton"
                            class="mdl-button mdl-js-button mdl-button--fab mdl-button--mini-fab">
                            <i class="material-icons">mic_off</i>
                        </button>
                    </div>

                    <!-- Screen Share Button -->
                    <div class="button-group">
                        <button id="screenShareButton" 
                                class="mdl-button mdl-js-button mdl-button--fab mdl-button--mini-fab">
                            <i class="material-icons">screen_share</i>
                        </button>
                    </div>

                    <!-- Video Element -->
                    <video id="videoElement" autoplay style="width: 800px; height: 600px;"></video>

                    <!-- Hidden Canvas -->
                    <canvas id="canvasElement" style="width: 800px; height: 600px;"></canvas>
                    <!-- Text Output -->
                    <div id="chatLog"></div>
                </div>
            </div>
        </main>
    </div>

    <script defer>
        // Audio context and stream management
        let audioContext = null;
        let audioStream = null;
        let workletInitialized = false;
        let audioQualityMonitor = {
            lastCheckTime: 0,
            sampleCount: 0,
            clippingCount: 0,
            underrunCount: 0,
            overrunCount: 0
        };
        
        // Initialize AudioContext with proper sample rate handling
        async function initAudioContext(preferredSampleRate = 16000) {
            try {
                if (!audioContext) {
                    // Try to match device capabilities
                    const devices = await navigator.mediaDevices.enumerateDevices();
                    const audioDevice = devices.find(device => device.kind === 'audioinput');
                    
                    if (audioDevice) {
                        const constraints = {
                            audio: {
                                deviceId: audioDevice.deviceId,
                                sampleRate: { ideal: preferredSampleRate }
                            }
                        };
                        
                        // Test device capabilities
                        const testStream = await navigator.mediaDevices.getUserMedia(constraints);
                        const track = testStream.getAudioTracks()[0];
                        const capabilities = track.getCapabilities();
                        testStream.getTracks().forEach(track => track.stop());
                        
                        // Use supported sample rate
                        const sampleRate = capabilities.sampleRate?.max >= preferredSampleRate ?
                            preferredSampleRate : capabilities.sampleRate?.max || 44100;
                        
                        console.log(`Using sample rate: ${sampleRate}Hz`);
                        
                        audioContext = new (window.AudioContext || window.webkitAudioContext)({
                            sampleRate: sampleRate,
                            latencyHint: 'interactive'
                        });
                        
                        await initializeAudioWorklet();
                    } else {
                        throw new Error('No audio input device found');
                    }
                }
                
                if (audioContext.state === "suspended") {
                    await audioContext.resume();
                }
                
                return audioContext;
            } catch (error) {
                console.error('Error initializing audio context:', error);
                throw error;
            }
        }

        // Monitor audio quality
        function updateAudioQualityMetrics(audioData) {
            const now = performance.now();
            if (now - audioQualityMonitor.lastCheckTime > 1000) {
                const metrics = {
                    timestamp: new Date().toISOString(),
                    clippingRatio: audioQualityMonitor.clippingCount / audioQualityMonitor.sampleCount,
                    underrunCount: audioQualityMonitor.underrunCount,
                    overrunCount: audioQualityMonitor.overrunCount
                };
                
                console.log('Audio Quality Metrics:', metrics);
                
                // Reset counters
                audioQualityMonitor.lastCheckTime = now;
                audioQualityMonitor.sampleCount = 0;
                audioQualityMonitor.clippingCount = 0;
                audioQualityMonitor.underrunCount = 0;
                audioQualityMonitor.overrunCount = 0;
            }
            
            // Update metrics
            audioQualityMonitor.sampleCount += audioData.length;
            audioData.forEach(sample => {
                if (Math.abs(sample) > 0.95) {
                    audioQualityMonitor.clippingCount++;
                }
            });
        }

        const URL = `wss://${window.location.hostname}/ws`;
        const video = document.getElementById("videoElement");
        const canvas = document.getElementById("canvasElement");
        let context;
        let webSocket = null;
        let isWebSocketReady = false;

        // Initialize context here
        window.addEventListener("load", async () => {
            context = canvas.getContext("2d");
            await connect();
            setInterval(captureImage, 3000);
        });

        const startButton = document.getElementById('startButton');
        const stopButton = document.getElementById('stopButton');
        let stream = null;
        let currentFrameB64;
        let mediaRecorder = null;
        let audioWorkletNode = null;
        let inputWorkletNode = null;
        let interval = null;
        let initialized = false;

        function arrayBufferToBase64(buffer) {
            const bytes = new Uint8Array(buffer);
            let binary = '';
            for (let i = 0; i < bytes.length; i++) {
                binary += String.fromCharCode(bytes[i]);
            }
            return btoa(binary);
        }

        function base64ToArrayBuffer(base64) {
            const binary_string = atob(base64);
            const len = binary_string.length;
            const bytes = new Uint8Array(len);
            for (let i = 0; i < len; i++) {
                bytes[i] = binary_string.charCodeAt(i);
            }
            return bytes.buffer;
        }

        // Function to start screen capture
        async function startScreenShare() {
            try {
                if (!stream) {
                    stream = await navigator.mediaDevices.getDisplayMedia({
                        video: {
                            width: { max: 800 },
                            height: { max: 600 },
                        },
                    });

                    video.srcObject = stream;
                    await new Promise(resolve => {
                        video.onloadedmetadata = () => {
                            console.log("video loaded metadata");
                            resolve();
                        }
                    });

                    // Handle stream end
                    stream.getVideoTracks()[0].onended = () => {
                        stream = null;
                        video.srcObject = null;
                    };
                }
            } catch (err) {
                console.error("Error accessing the screen: ", err);
            }
        }

        // Function to capture an image from the shared screen
        function captureImage() {
            if (stream && video.videoWidth > 0 && video.videoHeight > 0 && context) {
                canvas.width = 640;
                canvas.height = 480;
                context.drawImage(video, 0, 0, canvas.width, canvas.height);
                const imageData = canvas.toDataURL("image/jpeg").split(",")[1].trim();
                currentFrameB64 = imageData;
            }
            else {
                console.log("no stream or video metadata not loaded");
            }
        }

        // Add screen share button handler
        document.getElementById('screenShareButton').addEventListener('click', async () => {
            await startScreenShare();
        });

        function connect() {
            return new Promise((resolve, reject) => {
                console.log("connecting: ", URL);

                if (webSocket && webSocket.readyState !== WebSocket.CLOSED) {
                    webSocket.close();
                }

                webSocket = new WebSocket(URL);

                webSocket.onclose = (event) => {
                    console.log("websocket closed: ", event);
                    isWebSocketReady = false;
                    setTimeout(() => connect(), 1000); // Attempt to reconnect after 1 second
                };

                webSocket.onerror = (event) => {
                    console.log("websocket error: ", event);
                    isWebSocketReady = false;
                };

                webSocket.onopen = async (event) => {
                    console.log("websocket open: ", event);
                    isWebSocketReady = true;
                    // Send setup message immediately after setting ready state
                    sendInitialSetupMessage();
                    resolve();
                };

                webSocket.onmessage = receiveMessage;
            });
        }

        function sendInitialSetupMessage() {
            if (!isWebSocketReady) {
                console.log("WebSocket not ready for setup message");
                return;
            }

            console.log("sending setup message");
            const setup_client_message = {
                setup: {
                    generation_config: { response_modalities: ["AUDIO"] },
                },
            };

            webSocket.send(JSON.stringify(setup_client_message));
        }

        function sendVoiceMessage(b64PCM) {
            if (!isWebSocketReady) {
                console.log("websocket not ready");
                return;
            }

            const payload = {
                realtime_input: {
                    media_chunks: [{
                        mime_type: "audio/pcm",
                        data: b64PCM,
                    },
                    {
                        mime_type: "image/jpeg",
                        data: currentFrameB64,
                    },
                    ],
                },
            };

            webSocket.send(JSON.stringify(payload));
            console.log("sent voice message");
        }

        function receiveMessage(event) {
            const messageData = JSON.parse(event.data);
            const response = new Response(messageData);

            if (response.text) {
                displayMessage("GEMINI: " + response.text);
            }
            if (response.audioData) {
                ingestAudioChunkToPlay(response.audioData);
            }
        }

        async function initializeAudioWorklet() {
            if (workletInitialized) return;

            try {
                await audioContext.audioWorklet.addModule("pcm-processor.js");
                
                // Create output worklet node
                audioWorkletNode = new AudioWorkletNode(audioContext, "pcm-processor", {
                    processorOptions: {
                        mode: 'output'
                    }
                });
                audioWorkletNode.connect(audioContext.destination);
                
                // Create input worklet node
                inputWorkletNode = new AudioWorkletNode(audioContext, "pcm-processor", {
                    processorOptions: {
                        mode: 'input'
                    }
                });
                inputWorkletNode.port.onmessage = (e) => {
                    if (e.data.type === 'audio_data') {
                        const base64 = arrayBufferToBase64(e.data.buffer);
                        sendVoiceMessage(base64);
                    }
                };
                
                initialized = true;
                console.log("Audio worklet initialized successfully");
            } catch (error) {
                console.error("Failed to initialize audio worklet:", error);
                workletInitialized = false;
            }
        }

        function convertPCM16LEToFloat32(pcmData) {
            const inputArray = new Int16Array(pcmData);
            const float32Array = new Float32Array(inputArray.length);

            for (let i = 0; i < inputArray.length; i++) {
                float32Array[i] = inputArray[i] / 32768;
            }

            return float32Array;
        }

        async function ingestAudioChunkToPlay(base64AudioChunk) {
            try {
                if (audioContext.state === "suspended") {
                    await audioContext.resume();
                }
                const arrayBuffer = base64ToArrayBuffer(base64AudioChunk);
                const float32Data = convertPCM16LEToFloat32(arrayBuffer);

                audioWorkletNode.port.postMessage(float32Data);
            } catch (error) {
                console.error("Error processing audio chunk:", error);
            }
        }

        async function startAudioInput() {
            try {
                // Initialize audio context with device-compatible sample rate
                await initAudioContext();

                // Close any existing audio stream
                if (audioStream) {
                    audioStream.getTracks().forEach(track => track.stop());
                }

                // Get supported constraints
                const deviceCapabilities = await navigator.mediaDevices.getSupportedConstraints();
                const audioConstraints = {
                    channelCount: 1,
                    echoCancellation: true,
                    noiseSuppression: true,
                    autoGainControl: true
                };

                // Add sample rate if supported
                if (deviceCapabilities.sampleRate) {
                    audioConstraints.sampleRate = { ideal: audioContext.sampleRate };
                }

                // Get new audio stream
                audioStream = await navigator.mediaDevices.getUserMedia({
                    audio: audioConstraints
                });

                // Create and configure audio source
                const source = audioContext.createMediaStreamSource(audioStream);
                
                // Add gain node for volume control
                const gainNode = audioContext.createGain();
                gainNode.gain.value = 1.0; // Adjustable gain
                source.connect(gainNode);
                gainNode.connect(inputWorkletNode);

                // Inform worklet about actual sample rate
                const streamSettings = audioStream.getAudioTracks()[0].getSettings();
                inputWorkletNode.port.postMessage({
                    type: 'set_sample_rate',
                    sampleRate: streamSettings.sampleRate
                });

                // Set up buffer processing with dynamic interval
                const bufferInterval = Math.floor(1000 * (2048 / audioContext.sampleRate));
                interval = setInterval(() => {
                    if (inputWorkletNode?.port) {
                        inputWorkletNode.port.postMessage({ type: 'get_buffer' });
                    }
                }, bufferInterval);

                // Add audio quality monitoring
                const analyzerNode = audioContext.createAnalyser();
                analyzerNode.fftSize = 2048;
                source.connect(analyzerNode);
                
                const monitorInterval = setInterval(() => {
                    const dataArray = new Float32Array(analyzerNode.frequencyBinCount);
                    analyzerNode.getFloatTimeDomainData(dataArray);
                    updateAudioQualityMetrics(dataArray);
                }, 1000);

                console.log(`Audio input started successfully:
                    Sample Rate: ${streamSettings.sampleRate}Hz
                    Buffer Interval: ${bufferInterval}ms
                    Monitoring: Active`);
                
            } catch (error) {
                console.error("Error starting audio input:", error);
                displayMessage("Error: Could not start audio input. Please check your microphone settings.");
                throw error;
            }
        }

        function stopAudioInput() {
            if (inputWorkletNode) {
                inputWorkletNode.disconnect();
            }
            if (audioStream) {
                audioStream.getTracks().forEach(track => track.stop());
                audioStream = null;
            }
            clearInterval(interval);
            interval = null;
            console.log("Audio input stopped");
        }

        function displayMessage(message) {
            console.log(message);
            addParagraphToDiv("chatLog", message);
        }

        function addParagraphToDiv(divId, text) {
            const newParagraph = document.createElement("p");
            newParagraph.textContent = text;
            const div = document.getElementById(divId);
            div.appendChild(newParagraph);
        }

        // Add click handlers with user gesture handling
        startButton.addEventListener('click', async () => {
            try {
                await startAudioInput();
            } catch (error) {
                console.error("Error starting audio:", error);
            }
        });

        stopButton.addEventListener('click', stopAudioInput);

        class Response {
            constructor(data) {
                this.text = null;
                this.audioData = null;
                this.endOfTurn = null;

                if (data.text) {
                    this.text = data.text
                }

                if (data.audio) {
                    this.audioData = data.audio;
                }
            }
        }
    </script>

</body>

</html>