<!DOCTYPE html>
<html>

<head>
    <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
    <link rel="stylesheet" href="https://code.getmdl.io/1.3.0/material.indigo-pink.min.css">
    <script defer src="https://code.getmdl.io/1.3.0/material.min.js"></script>

    <style>
        #videoElement {
            width: 800px;
            height: 600px;
            border-radius: 20px;
        }

        #canvasElement {
            display: none;
            width: 800px;
            height: 600px;
        }

        .demo-content {
            padding: 20px;
            display: flex;
            flex-direction: column;
            align-items: center;
        }

        .button-group {
            margin-bottom: 20px;
        }
    </style>
</head>

<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header">
        <header class="mdl-layout__header">
            <div class="mdl-layout__header-row">
                <!-- Title -->
                <span class="mdl-layout-title">Gemini Live Demo</span>
            </div>
        </header>
        <main class="mdl-layout__content">
            <div class="page-content">
                <div class="demo-content">
                    <!-- Voice Control Buttons -->
                    <div class="button-group">
                        <button id="startButton"
                            class="mdl-button mdl-js-button mdl-button--fab mdl-button--mini-fab mdl-button--colored">
                            <i class="material-icons">mic</i>
                        </button>
                        <button id="stopButton"
                            class="mdl-button mdl-js-button mdl-button--fab mdl-button--mini-fab">
                            <i class="material-icons">mic_off</i>
                        </button>
                    </div>

                    <!-- Screen Share Button -->
                    <div class="button-group">
                        <button id="screenShareButton" 
                                class="mdl-button mdl-js-button mdl-button--fab mdl-button--mini-fab">
                            <i class="material-icons">screen_share</i>
                        </button>
                    </div>

                    <!-- Video Element -->
                    <video id="videoElement" autoplay style="width: 800px; height: 600px;"></video>

                    <!-- Hidden Canvas -->
                    <canvas id="canvasElement" style="width: 800px; height: 600px;"></canvas>
                    <!-- Text Output -->
                    <div id="chatLog"></div>
                </div>
            </div>
        </main>
    </div>

    <script defer>
        // Single AudioContext instance for both input and output
        let audioContext = null;
        let audioStream = null;
        let workletInitialized = false;
        
        // Initialize AudioContext on user gesture
        async function initAudioContext(sampleRate = 16000) {
            if (audioContext) {
                await audioContext.close();
            }
            
            audioContext = new (window.AudioContext || window.webkitAudioContext)({
                sampleRate: sampleRate
            });
            
            if (audioContext.state === "suspended") {
                await audioContext.resume();
            }

            // Re-initialize worklet nodes with new context
            workletInitialized = false;
            await initializeAudioWorklet();
            
            return audioContext;
        }

        const URL = `wss://${window.location.hostname}/ws`;
        const video = document.getElementById("videoElement");
        const canvas = document.getElementById("canvasElement");
        let context;
        let webSocket = null;
        let isWebSocketReady = false;

        // Initialize context here
        window.addEventListener("load", async () => {
            context = canvas.getContext("2d");
            await connect();
            setInterval(captureImage, 3000);
        });

        const startButton = document.getElementById('startButton');
        const stopButton = document.getElementById('stopButton');
        let stream = null;
        let currentFrameB64;
        let mediaRecorder = null;
        let audioWorkletNode = null;
        let inputWorkletNode = null;
        let interval = null;

        // Function to start screen capture
        async function startScreenShare() {
            try {
                if (!stream) {
                    stream = await navigator.mediaDevices.getDisplayMedia({
                        video: {
                            width: { max: 800 },
                            height: { max: 600 },
                        },
                    });

                    video.srcObject = stream;
                    await new Promise(resolve => {
                        video.onloadedmetadata = () => {
                            console.log("video loaded metadata");
                            resolve();
                        }
                    });

                    // Handle stream end
                    stream.getVideoTracks()[0].onended = () => {
                        stream = null;
                        video.srcObject = null;
                    };
                }
            } catch (err) {
                console.error("Error accessing the screen: ", err);
            }
        }

        // Function to capture an image from the shared screen
        function captureImage() {
            if (stream && video.videoWidth > 0 && video.videoHeight > 0 && context) {
                canvas.width = 640;
                canvas.height = 480;
                context.drawImage(video, 0, 0, canvas.width, canvas.height);
                const imageData = canvas.toDataURL("image/jpeg").split(",")[1].trim();
                currentFrameB64 = imageData;
            }
            else {
                console.log("no stream or video metadata not loaded");
            }
        }

        // Add screen share button handler
        document.getElementById('screenShareButton').addEventListener('click', async () => {
            await startScreenShare();
        });

        function connect() {
            return new Promise((resolve, reject) => {
                console.log("connecting: ", URL);

                if (webSocket && webSocket.readyState !== WebSocket.CLOSED) {
                    webSocket.close();
                }

                webSocket = new WebSocket(URL);

                webSocket.onclose = (event) => {
                    console.log("websocket closed: ", event);
                    isWebSocketReady = false;
                    setTimeout(() => connect(), 1000); // Attempt to reconnect after 1 second
                };

                webSocket.onerror = (event) => {
                    console.log("websocket error: ", event);
                    isWebSocketReady = false;
                };

                webSocket.onopen = async (event) => {
                    console.log("websocket open: ", event);
                    // Wait a brief moment before sending setup message
                    await new Promise(resolve => setTimeout(resolve, 100));
                    sendInitialSetupMessage();
                    isWebSocketReady = true;
                    resolve();
                };

                webSocket.onmessage = receiveMessage;
            });
        }

        function sendInitialSetupMessage() {
            if (!isWebSocketReady) {
                console.log("WebSocket not ready for setup message");
                return;
            }

            console.log("sending setup message");
            const setup_client_message = {
                setup: {
                    generation_config: { response_modalities: ["AUDIO"] },
                },
            };

            webSocket.send(JSON.stringify(setup_client_message));
        }

        function sendVoiceMessage(b64PCM) {
            if (!isWebSocketReady) {
                console.log("websocket not ready");
                return;
            }

            const payload = {
                realtime_input: {
                    media_chunks: [{
                        mime_type: "audio/pcm",
                        data: b64PCM,
                    },
                    {
                        mime_type: "image/jpeg",
                        data: currentFrameB64,
                    },
                    ],
                },
            };

            webSocket.send(JSON.stringify(payload));
            console.log("sent voice message");
        }

        function receiveMessage(event) {
            const messageData = JSON.parse(event.data);
            const response = new Response(messageData);

            if (response.text) {
                displayMessage("GEMINI: " + response.text);
            }
            if (response.audioData) {
                ingestAudioChunkToPlay(response.audioData);
            }
        }

        async function initializeAudioWorklet() {
            if (workletInitialized) return;

            try {
                await audioContext.audioWorklet.addModule("pcm-processor.js");
                
                // Create output worklet node for playback
                audioWorkletNode = new AudioWorkletNode(audioContext, "pcm-processor", {
                    processorOptions: {
                        mode: 'output'
                    }
                });
                audioWorkletNode.connect(audioContext.destination);
                
                // Create input worklet node for recording
                inputWorkletNode = new AudioWorkletNode(audioContext, "pcm-processor", {
                    processorOptions: {
                        mode: 'input'
                    }
                });
                
                inputWorkletNode.port.onmessage = (e) => {
                    if (e.data.type === 'audio_data') {
                        // Convert raw buffer to base64 in main thread
                        const base64 = arrayBufferToBase64(e.data.buffer);
                        sendVoiceMessage(base64);
                    }
                };
                
                workletInitialized = true;
                console.log("Audio worklet initialized successfully");
            } catch (error) {
                console.error("Failed to initialize audio worklet:", error);
                workletInitialized = false;
            }
        }

        function arrayBufferToBase64(buffer) {
            const bytes = new Uint8Array(buffer);
            let binary = '';
            for (let i = 0; i < bytes.length; i++) {
                binary += String.fromCharCode(bytes[i]);
            }
            return btoa(binary);
        }

        function base64ToArrayBuffer(base64) {
            const binaryString = window.atob(base64);
            const bytes = new Uint8Array(binaryString.length);
            for (let i = 0; i < binaryString.length; i++) {
                bytes[i] = binaryString.charCodeAt(i);
            }
            return bytes.buffer;
        }

        function convertPCM16LEToFloat32(pcmData) {
            const inputArray = new Int16Array(pcmData);
            const float32Array = new Float32Array(inputArray.length);

            for (let i = 0; i < inputArray.length; i++) {
                float32Array[i] = inputArray[i] / 32768;
            }

            return float32Array;
        }

        async function ingestAudioChunkToPlay(base64AudioChunk) {
            try {
                if (!audioContext || audioContext.state === "closed") {
                    await initAudioContext();
                }
                const arrayBuffer = base64ToArrayBuffer(base64AudioChunk);
                const float32Data = convertPCM16LEToFloat32(arrayBuffer);
                audioWorkletNode.port.postMessage(float32Data);
            } catch (error) {
                console.error("Error processing audio chunk:", error);
            }
        }

        async function startAudioInput() {
            try {
                // Try with exact sample rate first
                audioStream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        channelCount: 1,
                        sampleRate: { exact: 16000 },
                        echoCancellation: true,
                        noiseSuppression: true,
                    },
                });

                await initAudioContext(16000);
                const source = audioContext.createMediaStreamSource(audioStream);
                source.connect(inputWorkletNode);

            } catch (error) {
                console.error("Error starting audio input:", error);
                
                if (error.name === 'OverconstrainedError') {
                    try {
                        // Get audio stream with default sample rate
                        audioStream = await navigator.mediaDevices.getUserMedia({
                            audio: {
                                channelCount: 1,
                                echoCancellation: true,
                                noiseSuppression: true,
                            },
                        });

                        // Get the actual sample rate
                        const streamSettings = audioStream.getAudioTracks()[0].getSettings();
                        console.log("Using native sample rate:", streamSettings.sampleRate);
                        
                        // Create new context with matching sample rate
                        await initAudioContext(streamSettings.sampleRate);
                        const source = audioContext.createMediaStreamSource(audioStream);
                        source.connect(inputWorkletNode);

                    } catch (fallbackError) {
                        console.error("Error starting audio with fallback:", fallbackError);
                        return;
                    }
                }
            }

            // Start the audio processing interval
            interval = setInterval(() => {
                if (inputWorkletNode && inputWorkletNode.port) {
                    inputWorkletNode.port.postMessage({ type: 'get_buffer' });
                }
            }, 1000);

            console.log("Audio input started successfully");
        }

        function stopAudioInput() {
            if (audioStream) {
                audioStream.getTracks().forEach(track => track.stop());
                audioStream = null;
            }
            if (inputWorkletNode) {
                inputWorkletNode.disconnect();
            }
            clearInterval(interval);
            interval = null;
            console.log("Audio input stopped");
        }

        function displayMessage(message) {
            console.log(message);
            addParagraphToDiv("chatLog", message);
        }

        function addParagraphToDiv(divId, text) {
            const newParagraph = document.createElement("p");
            newParagraph.textContent = text;
            const div = document.getElementById(divId);
            div.appendChild(newParagraph);
        }

        // Add click handlers with user gesture handling
        startButton.addEventListener('click', async () => {
            try {
                await startAudioInput();
            } catch (error) {
                console.error("Error starting audio:", error);
            }
        });

        stopButton.addEventListener('click', stopAudioInput);

        class Response {
            constructor(data) {
                this.text = null;
                this.audioData = null;
                this.endOfTurn = null;

                if (data.text) {
                    this.text = data.text
                }

                if (data.audio) {
                    this.audioData = data.audio;
                }
            }
        }
    </script>

</body>

</html>